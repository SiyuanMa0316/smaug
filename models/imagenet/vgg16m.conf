// VGG 16 network with the two FC of depth 4096 removed and replaced with
// another maxpool and conv layer.
//
// Changpinyo, Soravit, et al. "The Power of Sparsity in Convolutional Neural
// Networks", Under review at ICLR 2017.

network {
  name = "vgg16"
  input_rows = 224
  input_cols = 224
  input_height = 3

  layer conv0 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 64
      pad = 1
      kernel_size = 3
    }
  }

  layer conv1 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 64
      pad = 1
      kernel_size = 3
    }
  }

  layer pool2 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }

  layer conv3 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 128
      pad = 1
      kernel_size = 3
    }
  }

  layer conv4 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 128
      pad = 1
      kernel_size = 3
    }
  }

  layer pool5 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }

  layer conv6 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 256
      pad = 1
      kernel_size = 3
    }
  }

  layer conv7 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 256
      pad = 1
      kernel_size = 3
    }
  }

  layer conv8 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 256
      pad = 0
      kernel_size = 1
    }
  }

  layer pool9 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }

  layer conv10 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 3
    }
  }

  layer conv11 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 3
    }
  }

  layer conv12 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 1
    }
  }

  layer pool13 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }

  layer conv14 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 3
    }
  }

  layer conv15 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 3
    }
  }

  layer conv16 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 512
      pad = 0
      kernel_size = 1
    }
  }

  layer pool17 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }


  layer conv18 {
    type = CONVOLUTION
    activation = RELU
    convolution_param {
      num_output = 1024
      pad = 1
      kernel_size = 3
    }
  }

  layer pool19 {
    type = POOLING
    activation = RELU
    pooling_param {
      pool = MAX
      size = 2
      stride = 2
    }
  }

  layer fc20 {
    type = INNER_PRODUCT
    inner_product_param {
      num_output = 1000
    }
  }
}
